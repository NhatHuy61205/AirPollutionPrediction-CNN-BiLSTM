{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4569c2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.  ]\n",
      " [0.25]\n",
      " [0.5 ]\n",
      " [0.75]\n",
      " [1.  ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "# D·ªØ li·ªáu gi·∫£ ƒë·ªãnh\n",
    "data = np.array([[1], [2], [3], [4], [5]])\n",
    "\n",
    "# Kh·ªüi t·∫°o v√† fit scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32da7f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Gi·∫£ s·ª≠ b·∫°n c√≥ 5 m·∫´u, m·ªói m·∫´u 10 th·ªùi ƒëi·ªÉm, m·ªói th·ªùi ƒëi·ªÉm 3 bi·∫øn\n",
    "samples = 5\n",
    "seq_length = 10\n",
    "input_features = 3\n",
    "\n",
    "# T·∫°o d·ªØ ki·ªán ng·∫´u nhi√™n\n",
    "X = np.random.rand(samples, seq_length, input_features).astype(np.float32)\n",
    "# Gi·∫£ s·ª≠ m·ªói m·∫´u ch·ªâ c√≥ 1 output (v√≠ d·ª• PM2.5)\n",
    "y = np.random.rand(samples, 1).astype(np.float32)\n",
    "\n",
    "# Chuy·ªÉn sang Tensor\n",
    "X_tensor = torch.tensor(X)\n",
    "y_tensor = torch.tensor(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ea0631b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)  # (batch, seq_length, hidden_dim)\n",
    "        out = out[:, -1, :]    # L·∫•y th·ªùi ƒëi·ªÉm cu·ªëi c√πng\n",
    "        return self.fc(out)   # (batch, 1)\n",
    "\n",
    "\n",
    "# Kh·ªüi t·∫°o m√¥ h√¨nh\n",
    "input_size = input_features  # = 3\n",
    "hidden_size = 64\n",
    "model = LSTMModel(input_size, hidden_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "32acaf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted shape : torch.Size([5, 1])\n",
      "Predicted values : tensor([[0.0961],\n",
      "        [0.1111],\n",
      "        [0.0977],\n",
      "        [0.0929],\n",
      "        [0.1000]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Forward 1 batch d·ªØ ki·ªán\n",
    "pred = model(X_tensor)\n",
    "\n",
    "print(\"Predicted shape :\", pred.shape)\n",
    "print(\"Predicted values :\", pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b71b0b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:\n",
      "LSTMModel(\n",
      "  (lstm): LSTM(3, 64, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Output:\n",
      "tensor([[-0.0795],\n",
      "        [-0.0122],\n",
      "        [-0.0005],\n",
      "        [-0.0574],\n",
      "        [-0.0106]], grad_fn=<AddmmBackward0>)\n",
      "Output shape : torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Gi·∫£ s·ª≠ m·ªói th·ªùi ƒëi·ªÉm b·∫°n c√≥ 3 bi·∫øn (input_features = 3)\n",
    "# b·∫°n mu·ªën LSTM ghi nh·ªõ nh·ªØng th√¥ng tin v·ªÅ th·ªùi gian\n",
    "# sau khi x·ª≠ l√Ω, b·∫°n ch·ªâ d·ª± ƒëo√°n 1 gi√° tr·ªã (v√≠ d·ª•: PM2.5)\n",
    "\n",
    "\n",
    "# ƒê√¢y ch√≠nh l√† m√¥ h√¨nh LSTM ho√†n ch·ªânh\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        # G·ªçi h√†m kh·ªüi ƒë·ªông c·ªßa cha (super) ƒë·ªÉ kh·ªüi ƒë·ªông nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        # ƒê√¢y l√† LSTM - d√≤ng n√†y s·∫Ω ƒë∆∞·ª£c g√°n v√†o bi·∫øn c·ªßa ƒë·ªëi t∆∞·ª£ng\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        # Ti·∫øp theo th√™m 1 t·∫ßng fully connected ƒë·ªÉ bi·∫øn t·ª´ hidden sang output\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # 1 gi√° tr·ªã t·∫°i m·ªói th·ªùi ƒëi·ªÉm cu·ªëi c√πng\n",
    "\n",
    "    # H√†m forward (ƒë√¢y ch√≠nh l√† d√≤ng x·ª≠ l√Ω ch√≠nh khi b·∫°n truy·ªÅn d·ªØ ki·ªán v√†o m√¥ h√¨nh)\n",
    "    def forward(self, x):\n",
    "        # x k√≠ch th∆∞·ªõc [batch, seq_length, input_features]\n",
    "\n",
    "        # LSTM tr·∫£ v·ªÅ:\n",
    "        # - out: output t·∫°i m·ªói th·ªùi ƒëi·ªÉm\n",
    "        # - (h, c): hidden state & cell state (nh∆∞ng n·∫øu b·∫°n ch∆∞a d√πng th√¨ ch·ªâ c·∫ßn _)\n",
    "\n",
    "        out, _ = self.lstm(x)  # (batch, seq_length, hidden_dim)\n",
    "\n",
    "        # Ch·ªâ quan t√¢m th·ªùi ƒëi·ªÉm cu·ªëi c√πng\n",
    "        out = out[:, -1, :]    # (batch, hidden_dim)\n",
    "\n",
    "        # Sau khi c√≥ ƒë∆∞·ª£c d√≤ng cu·ªëi c√πng, ƒë∆∞a qua fully connected ƒë·ªÉ ra output\n",
    "        return self.fc(out)   # (batch, 1)\n",
    "\n",
    "\n",
    "# Gi·∫£ s·ª≠ b·∫°n c√≥:\n",
    "input_features = 3\n",
    "hidden_dim = 64\n",
    "\n",
    "# T·∫°o m√¥ h√¨nh\n",
    "model = LSTMModel(input_features, hidden_dim)\n",
    "print(\"Model:\")\n",
    "print(model)\n",
    "\n",
    "\n",
    "# T·∫°o d·ªØ ki·ªán m·∫´u\n",
    "# 5 m·∫´u, m·ªói m·∫´u 10 th·ªùi ƒëi·ªÉm, m·ªói th·ªùi ƒëi·ªÉm 3 bi·∫øn\n",
    "batch = 5\n",
    "seq_length = 10\n",
    "X = torch.randn(batch, seq_length, input_features)\n",
    "\n",
    "# Forward ƒë·ªÉ ki·ªÉm ch·ª©ng\n",
    "output = model(X)\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(\"Output shape :\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc34b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTMModel(\n",
      "  (lstm): LSTM(3, 128, num_layers=2, batch_first=True)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n",
      "Output:\n",
      "tensor([[0.0331],\n",
      "        [0.0330],\n",
      "        [0.0276],\n",
      "        [0.0324],\n",
      "        [0.0297]], grad_fn=<AddmmBackward0>)\n",
      "Output shape : torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# Thay s·ªë t·∫ßng ho·∫∑c s·ªë n∆°ron ·∫©n t·∫°i ƒë√¢y\n",
    "input_features = 3\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "\n",
    "model = LSTMModel(input_features, hidden_dim, num_layers)\n",
    "print(model)\n",
    "batch = 5\n",
    "seq_length = 10\n",
    "X = torch.randn(batch, seq_length, input_features)\n",
    "\n",
    "# Forward ƒë·ªÉ ki·ªÉm ch·ª©ng\n",
    "output = model(X)\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "print(\"Output shape :\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "101106c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([[1, 2, 3, 4]])\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([1, 2, 3, 4])\n",
    "print(x.shape)         # torch.Size([4])\n",
    "\n",
    "# Chuy·ªÉn th√†nh tensor 2 chi·ªÅu: 1 h√†ng, 4 c·ªôt\n",
    "x_viewed = x.view(1, -1)\n",
    "print(x_viewed)        # tensor([[1, 2, 3, 4]])\n",
    "print(x_viewed.shape)  # torch.Size([1, 4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1baef132",
   "metadata": {},
   "source": [
    "User Manual :train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8a8341",
   "metadata": {},
   "source": [
    " train_loader (t·∫≠p hu·∫•n luy·ªán)<br>\n",
    "üìå D√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh: c·∫≠p nh·∫≠t tr·ªçng s·ªë b·∫±ng thu·∫≠t to√°n t·ªëi ∆∞u h√≥a (v√≠ d·ª• SGD, Adam)\n",
    "<br>\n",
    "‚úÖ Input: model, optimizer, loss_fn\n",
    "<br>\n",
    "‚úÖ Output: m√¥ h√¨nh h·ªçc ƒë∆∞·ª£c t·ª´ d·ªØ li·ªáu\n",
    "<br>\n",
    "2. val_loader (t·∫≠p ki·ªÉm tra/gi√°m s√°t)<br>\n",
    "üìå D√πng ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh trong qu√° tr√¨nh hu·∫•n luy·ªán, nh∆∞ng kh√¥ng c·∫≠p nh·∫≠t tr·ªçng s·ªë.\n",
    "<br>\n",
    "‚úÖ Gi√∫p b·∫°n ki·ªÉm tra: m√¥ h√¨nh c√≥ h·ªçc \"th·∫≠t\" hay ƒëang overfitting kh√¥ng?\n",
    "<br>\n",
    "‚úÖ ƒê∆∞·ª£c d√πng sau m·ªói epoch ho·∫∑c v√†i epoch\n",
    "<br>\n",
    "3. test_loader (t·∫≠p ki·ªÉm tra cu·ªëi c√πng)<br>\n",
    "üìå D√πng ƒë·ªÉ ƒë√°nh gi√° m√¥ h√¨nh cu·ªëi c√πng sau khi hu·∫•n luy·ªán xong.\n",
    "<br>\n",
    "‚úÖ K·∫øt qu·∫£ ·ªü test_loader cho bi·∫øt m√¥ h√¨nh t·ªïng qu√°t h√≥a t·ªët kh√¥ng\n",
    "<br>\n",
    "‚úÖ Kh√¥ng ƒë∆∞·ª£c d√πng ƒë·ªÉ ch·ªçn m√¥ h√¨nh (ƒë·ªÉ tr√°nh \"leak\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90d0b47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
